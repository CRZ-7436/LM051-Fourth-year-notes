#### Q1:

a) What is Reinforcement Learning?
What are the key issues that an RL agent must address?
Illustrate the discussion with an example.

**Reinforcement Learning (RL)** is a branch of machine learning where an agent learns to make a sequence of decisions to maximize a cumulative reward while interacting with an environment.

**Key Issues in RL:**

1. **Exploration vs. Exploitation:** Balancing between trying new actions and exploiting known ones.
    
2. **Credit Assignment:** Determining which actions contribute to rewards.
    

**Example:** In chess, an RL agent explores moves to win (exploration) and chooses moves based on experience (exploitation). It assigns credit to winning moves.



b) Explain how Temporal Difference (TD) methods bridge Dynamic Programming (DP)
and Monte Carlo (MC) methods. Include a backup diagram for DP in the discussion.

- **Dynamic Programming (DP)** is represented as the starting point, emphasizing its role in solving problems by breaking them down into simpler subproblems. It uses a state-value function update based on the Bellman Equation.
- **Monte Carlo (MC) Methods** are shown as another starting point, highlighting their approach of estimating values based on sample returns. These methods do not require a model of the environment and are model-free.
- **Temporal Difference (TD) Methods** are depicted as the central node, combining elements of both DP and MC methods. This illustrates the bridging role of TD methods between DP and MC.
- The diagram also includes a node labeled **"Bridge between DP and MC"** to emphasize the integrative function of TD methods.

![[Pasted image 20231214184418.png]]

C) What is meant by the terms on-policy and off-policy in the context of TD methods?
The discussion should include the equations for an on policy and off policy update

**On-Policy vs. Off-Policy in TD Methods:**

- **On-Policy:** On-policy methods learn the value of the policy that they are currently using to interact with the environment. They update their value estimates based on the same policy they follow.
    
    **On-Policy Update Equation:**
    
    css
    

- `Q(s, a) ← Q(s, a) + α * [R + γ * Q(s', a') - Q(s, a)]`
    
    - `Q(s, a)` is the action-value function.
    - `α` is the learning rate.
    - `R` is the immediate reward.
    - `γ` is the discount factor.
    - `s'` is the next state.
    - `a'` is the next action.
- **Off-Policy:** Off-policy methods learn the value of a target policy different from the policy they use for exploration. They update their value estimates based on experiences generated by following a different (possibly exploratory) policy.
    
    **Off-Policy Update Equation (Q-learning):**
    
    css
    

- `Q(s, a) ← Q(s, a) + α * [R + γ * max(Q(s', a')) - Q(s, a)]`
    
    - `max(Q(s', a'))` selects the action with the highest estimated value in the next state.

In summary, on-policy TD methods update value estimates based on their current policy, while off-policy TD methods, like Q-learning, update value estimates based on a different policy, allowing for more flexible and potentially more efficient learning.


d) Figure Question-1 shows the paths learned by Sarsa and Q for the cliff walking
problem from Sutton and Barto (2018). Identify the path learned by Sarsa and Q.
Explain why the paths differ and refer to the update in part (c ) in the answer

In the cliff walking problem, the paths learned by Sarsa and Q-learning algorithms are different due to their inherent nature of learning policies.

- **Sarsa (On-policy Learning):** Sarsa is an on-policy learning algorithm, which means it learns the value of the policy being followed, including the exploration steps. The path learned by Sarsa tends to be more conservative and avoids the cliff more cautiously. This is because Sarsa takes into account the current policy's actions, including the exploratory moves, which might lead to falling off the cliff. Therefore, it learns a safer path that stays away from the cliff edge.
    
- **Q-learning (Off-policy Learning):** Q-learning, on the other hand, is an off-policy learning algorithm. It learns the value of the optimal policy, independently of the agent's actions. The path learned by Q-learning tends to be more optimal but riskier, as it directly learns the best possible policy, which is the shortest path to the goal. This path often goes closer to the cliff edge because Q-learning evaluates the best possible action at each state without considering the exploratory moves.
    

The difference in the paths learned by Sarsa and Q-learning can be attributed to their update rules:

**Sarsa Update Rule:**
![[Pasted image 20231214185259.png]]
- This update rule takes into account the action At+1At+1​ as per the current policy, which includes exploratory actions.

**Q-learning Update Rule:**
 ![[Pasted image 20231214185336.png]]
- This update rule considers the best possible action at the next state S$t+1$, regardless of the policy being followed, thus learning the optimal policy.


In summary, the path learned by Sarsa is more cautious due to its on-policy nature, while Q-learning learns a more optimal but riskier path due to its off-policy approach.




#### Q2:

a) How many weights and connections in the first hidden layer of AlexNet
given filters of size 11 x 11 that generate 96 feature maps from an input
of size [224, 224]? Please show the calculations. Why is the number of
connections greater than the number of weights?

**Given:**

- Filter size: 11×1111×11
- Number of feature maps (filters): 9696
- Input size: 224×224224×224
- Input depth: 33 (assuming RGB image)

**Calculating Weights:**

- Each filter has a size of 11×1111×11.
- Since the input depth is 33, each filter will have 11×11×311×11×3 weights.
- There are 9696 such filters.
- Therefore, the total number of weights is 11×11×3×9611×11×3×96.

**Calculating Connections:**

- Each unit in the feature map is connected to a 11×1111×11 region in the input.
- For each unit in the feature map, there are 11×11×311×11×3 connections (due to the input depth).
- The size of each feature map is determined by the input size and the filter size. Assuming stride 1 and no padding, the size of the feature map is (224−11+1)×(224−11+1)=214×214(224−11+1)×(224−11+1)=214×214.
- Therefore, the total number of connections is 11×11×3×214×214×9611×11×3×214×214×96.

**Why is the number of connections greater than the number of weights?**

- This is due to weight sharing in convolutional neural networks. Each filter (with its set of weights) is convolved across the entire input to produce a feature map. Thus, the same weights are used at every position of the input, leading to a large number of connections but a relatively smaller number of unique weights.

**Calculations:**

1. **Weights:** 11×11×3×96=34,84811×11×3×96=34,848 weights.
2. **Connections:** 11×11×3×214×214×96=164,299,77611×11×3×214×214×96=164,299,776 connections.

This calculation shows that while the number of unique weights in a convolutional layer is relatively small, the number of connections can be quite large due to the nature of the convolution operation and weight sharing.


B) Ioffe and Szegedy (2015) proposed Batch Normalisation as a mechanism to
reduce the impact of vanishing gradients. Describe the concept of vanishing
gradients and explain how this phenomenon arises. How many parameters in
the two Batch Normalisation layers in Figure Question-2? Of these, how many
are trainable?

![[Pasted image 20231214190126.png]]

**Vanishing Gradients:** Vanishing gradients is a problem in deep neural networks where gradients become increasingly smaller as they propagate back through the layers during training. This occurs especially in networks with many layers and/or with saturating activation functions like sigmoid or tanh. As a result, the weights in the earlier layers of the network receive very small updates, slowing down the training process or stopping it altogether.

**How Batch Normalization Helps:** Batch Normalization (proposed by Ioffe and Szegedy, 2015) addresses this issue by normalizing the inputs to each layer. It adjusts and scales the activations of the previous layer, ensuring that they have a mean close to 0 and a standard deviation close to 1. This normalization stabilizes the learning process and helps mitigate the problem of vanishing gradients.

**Parameters in Batch Normalization Layers:** In the given model, there are two Batch Normalization layers. Each Batch Normalization layer has two sets of parameters: gamma (scale) and beta (shift). These parameters are learned during training. Additionally, Batch Normalization maintains the moving average of mean and variance, which are used during inference but are not trainable.

Given the model architecture:

- Each Batch Normalization layer normalizes a different number of features: the first after a Flatten layer and the second after a Dense layer with 300 units.
- Therefore, the first Batch Normalization layer has parameters related to the flattened input (784 features from a 28x28 input), and the second has parameters related to the 300 units from the previous Dense layer.
- For each feature, there are 2 trainable parameters (gamma and beta), so the total number of trainable parameters in each Batch Normalization layer is twice the number of features normalized by that layer.

**Calculating Parameters:**

1. **First Batch Normalization Layer:** 2×784=15682×784=1568 trainable parameters.
2. **Second Batch Normalization Layer:** 2×300=6002×300=600 trainable parameters.

**Total Trainable Parameters in Batch Normalization Layers:** 1568+600=21681568+600=2168 trainable parameters.

C) Describe how the estimate and target values are calculated in a Deep Q Network (DQN).
Illustrate the discussion with coding fragments or pseudocode

In a DQN, the key components are the estimate and target values used to train the network. These values are crucial for the Q-learning update rule, which is at the heart of DQN's learning process.

**Estimate Value:**

- The estimate value in a DQN is the current Q-value obtained from the network for a given state-action pair.
- It is calculated using the current state and the action chosen by the policy (which could be ε-greedy).
- The estimate value represents the network's current prediction of the expected cumulative future reward for taking that action in that state.

**Target Value:**

- The target value in a DQN is based on the Bellman equation and is used as the 'ground truth' for training the network.
- It is calculated as the immediate reward received after taking an action, plus the discounted maximum Q-value for the next state. This maximum Q-value is obtained from the target network, which is a periodically updated copy of the main network.
- The target value represents the actual reward received plus the expected reward for the best future action, as estimated by the target network.

**Pseudocode:**

python

 Assume we have a function 'choose_action' that selects an action using the ε-greedy policy
 Assume 'network' is our main DQN model and 'target_network' is the periodically updated copy
 Assume 'state' is the current state, 'next_state' is the state after taking the action
 Assume 'reward' is the reward received after taking the action
 Assume 'done' is a boolean indicating whether the next state is terminal

action = choose_action(state, network)
estimate_value = network.predict(state, action)

if done:
    target_value = reward
else:
    future_rewards = target_network.predict(next_state)
    max_future_reward = max(future_rewards)
    target_value = reward + gamma * max_future_reward

'gamma' is the discount factor, typically between 0.9 and 0.99


**Explanation:**

- The `choose_action` function selects an action based on the current policy.
- The `estimate_value` is calculated by the main network (`network`) using the current state and action.
- The `target_value` is calculated using the reward and the discounted maximum future reward. If the `next_state` is terminal (`done` is `True`), the target value is just the immediate reward.
- The `target_network` is used to predict future rewards to avoid moving target issues during training.

This approach allows the DQN to learn optimal policies by approximating the Q-function, which maps state-action pairs to expected future rewards.




####Q3) 

#### Q3:


**a)**
Explain why the number of parameters in GoogleLeNet using Inception
modules is significantly less than AlexNet - 6 million as opposed to 60 million.
The answer should focus exclusively on the Inception module.
Illustrate the answer with a diagram and calculations
The number of parameters in GoogleLeNet using Inception modules is significantly less than in AlexNet due to the efficient design of the Inception module, which reduces redundancy while maintaining model performance.

**Explanation:**

1. **Inception Module:** GoogleLeNet introduced the Inception module, which consists of multiple parallel convolutional layers of different kernel sizes (e.g., 1x1, 3x3, 5x5), as well as pooling layers. This design allows the network to capture features at different spatial scales.
    
2. **Parameter Efficiency:** The Inception module is parameter-efficient because it uses 1x1 convolutions to reduce the depth of the feature maps before applying larger convolutions. These 1x1 convolutions serve as bottleneck layers, reducing the number of input channels and, consequently, the number of parameters.
    
3. **Calculation:** Let's consider a simplified example. Suppose we have an input with 256 channels and we apply a 1x1 convolution followed by a 3x3 convolution in parallel, where each has 128 output channels. In this case, the number of parameters for the 1x1 convolution is 256 * 128 = 32,768, and for the 3x3 convolution is 128 * 256 * 9 = 294,912. Without the 1x1 bottleneck layer, the 3x3 convolution alone would require significantly more parameters.
    
4. **Reduction of Parameters:** By judiciously using these 1x1 bottleneck layers and parallel operations in the Inception module, GoogleLeNet achieves a good balance between model expressiveness and parameter efficiency. This design significantly reduces the number of parameters compared to earlier architectures like AlexNet, which used large fully connected layers, resulting in a high parameter count.
    

**Diagram (simplified):**

![[Pasted image 20231214190938.png]]

In this example, the 1x1 convolution acts as a bottleneck layer, reducing the number of channels before the 3x3 convolution.

In summary, GoogleLeNet's Inception module's parameter-efficient design, which incorporates 1x1 bottleneck layers, allows it to maintain model performance with significantly fewer parameters compared to AlexNet. This innovation in architecture optimization is a key reason for the reduced parameter count.


**b)** 
Regularistion is standard practice in deep learning.
What is regularisation?
Describe three regularisation techniques.
Include coding fragments or pseudocode and diagrams where relevant


**Regularization in Deep Learning:**

Regularization is a technique used in deep learning to prevent overfitting, which occurs when a neural network learns to fit the training data too closely, resulting in poor generalization to new, unseen data. Regularization introduces constraints on the network's parameters to promote simpler models that generalize better.

**Three Common Regularization Techniques:**

**1. L2 Regularization (Weight Decay):**

- **Description:** L2 regularization, also known as weight decay, adds a penalty term to the loss function, encouraging the model's weights to be smaller. It helps prevent overfitting by discouraging overly complex models.
    
- **Pseudocode:**
 Loss calculation
original_loss = compute_loss(y_true, y_pred)

 L2 regularization term
regularization_term = lambda * Σ(w^2)

 Total loss
total_loss = original_loss + regularization_term


**2. Dropout:**

- **Description:** Dropout is a technique that randomly deactivates (sets to zero) a fraction of neurons during each training iteration. This encourages the network to be more robust and prevents overreliance on specific neurons.
    
- **Pseudocode:**
- if training:
    dropout_mask = generate_dropout_mask()
    input = input * dropout_mask


**3. Early Stopping:**

- **Description:** Early stopping is a simple regularization method that monitors the model's performance on a validation dataset during training. Training is halted when the validation performance starts to degrade, preventing overfitting.
    
- **Pseudocode:**
- for each epoch:
    train_model()
    validation_loss = calculate_loss(validation_data)
    if validation_loss increases:
        stop_training()



c)
Describe the key concept(s) in ResNet. Include a discussion on the purpose
of using a kernel of size 1x1 with stride 2. Illustrate the discussion with a diagram

ResNet, short for Residual Network, is a deep neural network architecture known for its innovation in addressing the vanishing gradient problem and enabling the training of extremely deep networks. The key concept in ResNet is the use of residual blocks.

1. **Residual Blocks:** Residual blocks introduce skip connections (also known as identity shortcuts) that allow the network to bypass one or more layers. This enables the direct flow of gradients during training and mitigates the vanishing gradient problem. A residual block typically consists of two main paths:
    - The identity path: This path carries the input directly to the output through an identity function.
    - The residual path: This path computes the residual (the difference between the desired output and the identity path) and adds it back to the identity path.

**Purpose of Using a 1x1 Kernel with Stride 2:**

In ResNet, one common design choice is to use a convolutional layer with a 1x1 kernel and a stride of 2 in some residual blocks. This has a specific purpose:

- **Downsampling:** The 1x1 convolution with stride 2 reduces the spatial dimensions of the feature maps in the residual block. As a result, it reduces the spatial resolution and the computational cost while increasing the receptive field.

**Diagram:**

Here's a simplified diagram illustrating a residual block with a 1x1 convolutional layer with stride 2:
![[Pasted image 20231214192407.png]]

In this diagram:

- `Input` represents the input feature map.
- `Conv1x1Stride2` is a 1x1 convolutional layer with stride 2.
- `Conv3x3` is a 3x3 convolutional layer.
- `Sum` denotes element-wise addition.
- `Output` is the output feature map of the residual block.

The 1x1 convolution with stride 2 helps reduce the spatial dimensions before the subsequent 3x3 convolution. This allows ResNet to effectively downsample feature maps, capture more global information, and facilitate training of deep networks.




#### Q4: 
a)
Compare and contrast the Symbolic AI and Machine Learning (ML) paradigms,
and illustrate the answer with examples

**Symbolic AI vs. Machine Learning (ML):**

- **Symbolic AI:**
    
    - Relies on predefined rules and explicit knowledge representation.
    - Examples include expert systems with predefined rules (e.g., medical diagnosis).
    - Can be rigid and requires manual rule engineering.
    - More interpretable due to explicit rules.
    - Does not inherently depend on large datasets.
    - May struggle with handling uncertainty and context.
- **Machine Learning (ML):**
    
    - Learns patterns and relationships from data without explicit rules.
    - Examples include spam email filters learning from labeled data.
    - Adapts to new tasks with minimal human intervention.
    - Often considered black boxes, making interpretability challenging.
    - Heavily relies on data availability.
    - May not provide explanations for its decisions.


b)
Discuss four metric-based approaches used to quantify the performance
of an ML algorithm, one example being precision and recall

**1. Precision and Recall:**

- **Purpose:** Precision measures the accuracy of positive predictions, while recall quantifies the ability of the model to identify all relevant instances.
- **Example:** In a medical diagnosis system, precision would measure the proportion of correctly identified cases among all positive predictions, and recall would measure the proportion of correctly identified cases among all actual cases.

**2. Accuracy:**

- **Purpose:** Accuracy calculates the overall correctness of predictions by dividing the number of correct predictions by the total number of predictions.
- **Example:** In a sentiment analysis model, accuracy would assess the percentage of correctly classified sentiments (positive/negative) out of all samples.

**3. F1 Score:**

- **Purpose:** The F1 score is the harmonic mean of precision and recall, providing a balance between the two metrics. It is particularly useful when class distribution is imbalanced.
- **Example:** In a text classification task, the F1 score would give a combined assessment of precision and recall, considering both false positives and false negatives.

**4. Area Under the Receiver Operating Characteristic (ROC-AUC):**

- **Purpose:** ROC-AUC measures the model's ability to distinguish between positive and negative classes across various threshold settings. It evaluates the area under the ROC curve, with a higher value indicating better performance.
- **Example:** In a binary classification problem, ROC-AUC would assess the model's ability to rank positive samples higher than negative ones when adjusting the classification threshold.


c)
What is the update rule for a linear perceptron?
Write the code for a perceptron with fixed weights w1 = w2 =1, and bias -0.5.
Name the logical gate modelled by this perceptron

**Update Rule for a Linear Perceptron:**

The update rule for a linear perceptron is a simple weight adjustment based on the difference between the predicted output and the target output (also known as the error). 

**Code for a Perceptron with Fixed Weights:**

If you want to implement a perceptron with fixed weights `w1 = 1` and `w2 = 1` and bias `-0.5` in Python, it might look like this:

//Fixed weights and bias
w1 = 1
w2 = 1
bias = -0.5

//Input values
X1 = 0.7
X2 = 0.3

//Calculate the predicted output
predicted_output = w1 * X1 + w2 * X2 + bias

//Activation function (for binary classification)
if predicted_output > 0:
    perceptron_output = 1
else:
    perceptron_output = 0

print("Predicted Output:", perceptron_output)

**Logical Gate Modeled by this Perceptron:**

The perceptron you've described, with `w1 = 1`, `w2 = 1`, and `bias = -0.5`, models the behavior of an AND gate. An AND gate produces a `1` (or True) output only when both of its inputs are `1`, and it produces `0` (or False) otherwise. In this case, the perceptron will output `1` when both `X1` and `X2` are greater than or equal to `0.5` (due to the fixed weights and bias), which is the behavior of an AND gate.


