#### Lecture H: LeNet

- **LeNet CNN Architecture**: Covers the implementation of LeNet CNN, including hyperparameters, network definition, convolutional filters with ReLU activations, and max pooling.
- **Saturation and Vanishing Gradient**: Discusses the issues of saturation in sigmoid and tanh activation functions and the vanishing gradient problem in deep neural networks.
- **ReLU and ELU**: Introduces ReLU (Rectified Linear Unit) as a solution to the vanishing gradient problem and ELU (Exponential Linear Unit) for improving upon ReLU.
- **LeNet Structure**: Describes the second convolutional stage with ReLU activations, max pooling, flattening, and a dense network with a softmax classifier.

#### Lecture I: CNNs Part 3

- **Categorical Data**: Explains handling categorical data, including one-hot encoding and dummy variable encoding.
- **Softmax Activation**: Discusses the use of softmax activation for multiclass classification.
- **Cross Entropy**: Covers cross-entropy loss functions, including sparse categorical and categorical cross-entropy.
- **Regularization - Dropout**: Introduces dropout as a regularization technique to reduce overfitting, explaining its rationale and implementation.

#### Lecture J: CNNs Part 4

- **Machine Learning Challenges**: Discusses challenges such as insufficient data, non-representative data, poor quality data, irrelevant features, overfitting, and underfitting.
- **Testing and Validation**: Explains the importance of splitting data into training and test sets and using validation for model selection.
- **Measuring Performance**: Covers performance metrics like top-1 and top-5 accuracy, confusion matrix, precision, recall, ROC, and AUC.
- **Weight Initialization**: Discusses Glorot and He initialization methods for resolving unstable gradients in deep learning models.
- **VGG Net**: Briefly mentions VGG Net and its significance in deep learning.

### Key Takeaways for Exam Preparation

- **Understanding CNN Architectures**: Focus on the structure and function of CNNs, particularly LeNet, and their activation functions like ReLU and ELU.
- **Data Handling and Regularization**: Grasp the concepts of handling categorical data and the importance of regularization techniques like dropout in preventing overfitting.
- **Performance Metrics**: Familiarize yourself with various performance metrics used in evaluating neural networks, such as accuracy, precision, recall, and AUC.
- **Challenges in Machine Learning**: Understand the common challenges faced in machine learning, including data issues and model fitting problems.