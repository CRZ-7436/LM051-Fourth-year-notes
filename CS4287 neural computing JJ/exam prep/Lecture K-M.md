#### Lecture K: CNNs Part 5 - Architectures

- **Batch Normalisation**: Discusses the concept of Batch Normalisation (BN) introduced by Ioffe and Szegedy in 2015, its implementation in Keras, and its role in reducing vanishing gradients.
- **Gradient Clipping**: Covers the technique of gradient clipping to address the exploding gradient problem, particularly in Recurrent Neural Networks (RNNs).
- **Transfer Learning**: Explains the concept of transfer learning, its advantages, and standard practices in Deep Neural Networks (DNNs).
- **Adversarial Data**: Introduces the concept of adversarial data and its impact on neural network performance.
- **CNN Architectures**: Discusses various CNN architectures like AlexNet, GoogleLeNet, VGGNet, and ResNet, including their unique features and contributions to the field.

#### Lecture L: CNNs Part 6 - Architectures

- **ResNet-34 Implementation**: Provides details on implementing ResNet-34 using Keras, including the use of residual units and batch normalization.
- **Pretrained Models**: Discusses the use of pretrained models in Keras, such as ResNet-50, and the process of adapting these models for specific tasks.
- **Transfer Learning with Xception Model**: Describes the process of leveraging a pretrained Xception model for classifying a dataset of flowers, including data preprocessing and model adaptation.

#### Lecture M: Introduction to Reinforcement Learning (RL)

- **Motivation for RL**: Presents use cases like game playing (e.g., chess, Go) to illustrate the motivation for RL.
- **RL Challenges**: Discusses the balance between exploration and exploitation and the credit assignment problem in RL.
- **RL as a Markov Decision Process (MDP)**: Characterizes RL as an MDP, detailing its components like states, actions, and rewards.
- **RL Components**: Explains the elements of RL, including policies, reward signals, value functions, and the optional model of the environment.
- **Generalization in RL**: Highlights the need for generalization in RL, using examples like driving scenarios.
- **K-armed Bandit Problem**: Introduces the K-armed bandit problem as a simpler RL problem to understand the basics of action value estimation and exploration vs. exploitation.

### Key Takeaways for Exam Preparation

- **Understanding CNN Architectures**: Focus on the structure and function of various CNN architectures and their significance in deep learning.
- **Batch Normalization and Gradient Clipping**: Grasp the importance of techniques like batch normalization and gradient clipping in training neural networks.
- **Transfer Learning**: Understand the concept and application of transfer learning, especially using pretrained models for specific tasks.
- **Basics of Reinforcement Learning**: Familiarize yourself with the foundational concepts of RL, including the MDP framework, policy, reward, and value functions.