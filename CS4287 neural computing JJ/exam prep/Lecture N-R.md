#### Lecture N: RL – DP and MC

- **K-armed Bandit Problem**: Introduces the K-armed bandit problem and various action selection methods like greedy and ε-greedy.
- **Dynamic Programming (DP) and Monte Carlo (MC)**: Discusses the differences between DP and MC methods in Reinforcement Learning (RL), including bootstrapping and model requirements.
- **Value Functions**: Explains the concept of value functions in RL, including state value and state-action value functions.
- **Bellman’s Optimality Equation**: Introduces Bellman’s Optimality Equation and its application in RL.

#### Lecture O: RL – Temporal Difference (TD) Methods

- **Temporal Difference (TD) Learning**: Describes TD learning as a bridge between DP and MC, and introduces the TD update formula.
- **Sarsa and Q Learning**: Differentiates between on-policy Sarsa and off-policy Q Learning methods in TD learning.
- **Cliff Walking Example**: Analyzes the results of applying Sarsa and Q Learning to a cliff walking example.

#### Lecture P: RL – DQN and the Cartpole

- **TD Gammon**: Discusses TD Gammon, a reinforcement learning application for backgammon using TD learning.
- **Deep Q-Networks (DQN)**: Introduces DQNs and their application in solving the CartPole problem.
- **On vs. Off Policy Learning**: Explains the difference between on-policy and off-policy learning in the context of RL.

#### Lecture Q: RL – DQN for Atari

- **DQN Architecture for Atari**: Describes the architecture and loss function used in DQN for playing Atari games.
- **Double DQNs**: Introduces the concept of Double DQNs and their role in improving the performance of DQN.
- **Performance Evaluation**: Evaluates the performance of DQN on Atari games and compares it with other RL algorithms.

#### Lecture R: Module Summary and Final Exam

- **Module Summary**: Summarizes key topics covered in the module, including perceptrons, MLPs, CNNs, and RL.
- **Final Exam Details**: Provides information about the final exam format and grading.

### Key Takeaways for Exam Preparation

- **Understanding RL Methods**: Focus on the differences between DP, MC, and TD methods, and their applications in RL.
- **Temporal Difference Learning**: Grasp the concept of TD learning, especially Sarsa and Q Learning, and their practical applications.
- **Deep Q-Networks**: Understand DQNs and their significance in solving complex RL problems like CartPole and Atari games.
- **Module Overview**: Review the key concepts from the entire module, as the final exam will cover a broad range of topics.